# RAG-Powered Document Intelligence System

## Overview

This project is a production‑oriented **Retrieval‑Augmented Generation (RAG) system** designed to transform unstructured documents into an interactive, queryable knowledge interface. Users can upload their own documents and ask natural‑language questions, receiving grounded, source‑aware answers generated by a large language model.

The system prioritizes **local-first processing**, low operational cost, and modular design, while still supporting **real‑time web augmentation** when internal knowledge is insufficient.

---

## What the System Does

* Ingests user‑uploaded documents (PDF, text)
* Splits and embeds content into a semantic vector index
* Retrieves the most relevant chunks per query using similarity search
* Augments prompts with retrieved context
* Generates streamed, grounded answers using a high‑performance LLM
* Optionally enriches responses with live web search results

This enables accurate answers that are **contextual, explainable, and traceable to source documents**.

---

## How It Works (High Level)

1. **Document Ingestion**
   Uploaded files are parsed and split into overlapping text chunks to preserve semantic continuity.

2. **Embedding & Indexing**
   Each chunk is converted into a dense vector using a local embedding model and stored in a FAISS index for fast similarity search.

3. **Query Processing**
   User queries are embedded and matched against the vector index to retrieve the most relevant document segments.

4. **Context‑Aware Generation**
   Retrieved content is injected into a structured prompt and passed to a large language model for answer generation.

5. **Web Search Fallback (Optional)**
   If local context is insufficient, the system dynamically invokes web search to supplement responses with up‑to‑date information.

6. **Streaming Output**
   Responses are streamed token‑by‑token to the UI for a responsive chat experience.

---

## Architecture Highlights

* **Separation of concerns** across configuration, core RAG logic, tools, and UI
* **Local embeddings + vector store** for privacy and cost control
* **Pluggable LLM and search providers**
* **Session‑aware UI state** for multi‑turn conversations
* **Persistent vector index** to avoid reprocessing documents

The system follows SOLID principles and is designed to be easily extended or productionized.

---

## Tech Stack

* **Language**: Python 3.11+
* **LLM Inference**: Groq (Llama 3.1 70B)
* **Embeddings**: HuggingFace sentence‑transformers (local execution)
* **Vector Store**: FAISS
* **RAG Orchestration**: LangChain
* **UI**: Streamlit
* **Web Search**: Tavily API

All core retrieval and embedding operations run locally, minimizing external dependencies.

---

## Key Design Decisions

* **Local‑first embeddings** to eliminate recurring embedding costs
* **FAISS over hosted vector DBs** for simplicity and speed
* **Streaming generation** for better UX
* **Fallback search strategy** instead of unconditional web queries
* **Explicit configuration validation** to fail fast on misconfiguration

---

## Typical Use Cases

* Chat with internal documents (research papers, legal texts, manuals)
* Build a private knowledge assistant
* Prototype RAG systems without paid vector databases
* Learn production‑grade RAG architecture through real code

---

## Deployment

The application is designed for secure deployment using environment variables or secret managers. It runs reliably on:

* Streamlit Cloud
* Docker‑based environments
* Any cloud platform supporting Python web apps

No API keys or secrets are committed to source control.

---

## Scalability & Extension Paths

While currently optimized for single‑user or small‑scale usage, the architecture can be extended to support:

* Multi‑user document isolation
* Hosted vector databases (Pinecone, Weaviate)
* Authentication and access control
* Async ingestion pipelines
* API‑based (non‑UI) access

---

## Project Status

This project represents a **complete, working RAG system**, suitable for demos, portfolios, and further production hardening. It is not a toy example or notebook‑only prototype, but a modular application with clear boundaries and real‑world design considerations.

